{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "This file will show the process of downloading the data needed for this project.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Places](#1)\n",
    "- [Reviews](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is downloaded from [Google Local](https://cseweb.ucsd.edu/~jmcauley/datasets.html#google_local) data published by Professor Julian McAuley from University of California, San Diego. The data contains reviews about businesses along with their geographical location. The raw data has 3.116.785 businesses and 11.453.845 reviews, which spans across locations all over the world.\n",
    "\n",
    "The data is stored as multiple non comma seperated dictionaries and must be converted into readable json format. This process in done below. The code takes approximate 6 hours to run and outputs the two data files \"reviews.csv\" and \"places.csv\". In this data downloading process, we have decided to only use data from London, UK and Manhattan, US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are interested in the raw data and the issues contained in it, you are welcome to read the paragraph below. Otherwise, you'll find our code further below.**\n",
    "\n",
    "The Google Local data is comprised of dictionaries separated by new line operators. A sample of four lines from the `places.clean.json` file is shown below:\n",
    "```json\n",
    "{'name': u'Diamond Valley Lake Marina', 'price': None, 'address': [u'2615 Angler Ave', u'Hemet, CA 92545'], 'hours': [[u'Monday', [[u'6:30 am--4:15 pm']]], [u'Tuesday', [[u'6:30 am--4:15 pm']]], [u'Wednesday', [[u'6:30 am--4:15 pm']], 1], [u'Thursday', [[u'6:30 am--4:15 pm']]], [u'Friday', [[u'6:30 am--4:15 pm']]], [u'Saturday', [[u'6:30 am--4:15 pm']]], [u'Sunday', [[u'6:30 am--4:15 pm']]]], 'phone': u'(951) 926-7201', 'closed': False, 'gPlusPlaceId': '104699454385822125632', 'gps': [33.703804, -117.003209]}\n",
    "{'name': u'Blue Ribbon Cleaners', 'price': None, 'address': [u'Parole', u'Annapolis, MD'], 'hours': None, 'phone': u'(410) 266-6123', 'closed': False, 'gPlusPlaceId': '103054478949000078829', 'gps': [38.979759, -76.547538]}\n",
    "{'name': u'Portofino', 'price': None, 'address': [u'\\u0443\\u043b. \\u0422\\u0443\\u0442\\u0430\\u0435\\u0432\\u0430, 1', u'Nazran, Ingushetia, Russia', u'366720'], 'hours': [[u'Monday', [[u'9:30 am--9:00 pm']]], [u'Tuesday', [[u'9:30 am--9:00 pm']]], [u'Wednesday', [[u'9:30 am--9:00 pm']], 1], [u'Thursday', [[u'9:30 am--9:00 pm']]], [u'Friday', [[u'9:30 am--9:00 pm']]], [u'Saturday', [[u'9:30 am--9:00 pm']]], [u'Sunday', [[u'9:30 am--9:00 pm']]]], 'phone': u'8 (963) 173-38-38', 'closed': False, 'gPlusPlaceId': '109810290098030327104', 'gps': [43.22776, 44.762726]}\n",
    "{'name': u\"Dicola's Pizzeria\", 'price': None, 'address': [u'626 Can Do Expy # 1 , Hazle, PA 18202'], 'hours': None, 'phone': u'(570) 384-0520', 'closed': False, 'gPlusPlaceId': '104869934485244376571', 'gps': [40.9908, -76.0117]}\n",
    "```\n",
    "At first glance, this looks like dictionaries that should be simple to load into Python. However, the issue is that the data is not consistent on whether apostrophes or quotations marks are used to encapsulate strings in the dictionaries. For example, the first three lines are consistent with using apostrophes, which are not recognized as `json` format, but the fourth line uses quotation marks for the name since the name has an apostrophe in it, i.e. `Dicola's Pizzeria`.\n",
    "\n",
    "Therefore, the solution we found was to iterate over all the rows in the data files, convert the row to json format and write the row to a new file with comma separation. The solution is time-consuming and not very elegant, but since we are only running this once we went with it. The same treatment applies for the `reviews.clean.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "data_path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for unzipping gzip and saving as json\n",
    "def unzip_gzip(file):\n",
    "    with gzip.open(file, \"rb\") as f_in:\n",
    "        with open(file.rsplit(\".\", 1)[0], \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Places <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file path\n",
    "url = \"http://deepyeti.ucsd.edu/jmcauley/datasets/googlelocal/places.clean.json.gz\"\n",
    "filename = url.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download zip file from website\n",
    "with open(data_path + filename, \"wb\") as f:\n",
    "    r = requests.get(url)\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip and save locally\n",
    "unzip_gzip(data_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3114353it [04:42, 11020.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inspiration from https://gist.github.com/mbrzusto/23fe728966247f25f3ec\n",
    "fr=open(data_path + \"places.clean.json\")\n",
    "fw=open(data_path + \"places.json\", \"w\")\n",
    "written_firstline = 0\n",
    "for line in tqdm(fr):\n",
    "    json_dat = ast.literal_eval(line)\n",
    "    full_address = \", \".join(json_dat['address'])\n",
    "    in_ny = re.findall(r\"NY\\s\\d{5}\", json_dat['address'][-1]) # addresses in New York end with NY XXXXX\n",
    "    in_london = re.findall(r'London.*?United Kingdom', full_address) # addresses in London contains the word London followed by zip code of varying length and United Kingdom\n",
    "    if in_ny or in_london:\n",
    "        if written_firstline == 0: # If file is empty\n",
    "            fw.write(\"[\")\n",
    "            json.dump(json_dat, fw)\n",
    "            written_firstline += 1\n",
    "        else:\n",
    "            fw.write(\",\\n\")\n",
    "            json.dump(json_dat, fw)\n",
    "fw.write(\"]\")\n",
    "\n",
    "fw.close()\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert json file to csv\n",
    "with open(data_path + \"places.json\", \"r\") as f:\n",
    "    content = json.loads(f.read())\n",
    "\n",
    "df = pd.DataFrame(content)\n",
    "df.to_csv(data_path + \"places.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gPlusPlaceId` is a unique ID for each business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102851, 8)\n",
      "102851\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.gPlusPlaceId.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pd.read_csv(data_path + \"places.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download gzip data from website\n",
    "url = \"http://deepyeti.ucsd.edu/jmcauley/datasets/googlelocal/reviews.clean.json.gz\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "with open(data_path + filename, \"wb\") as f:\n",
    "    r = requests.get(url)\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip and save reviews locally\n",
    "unzip_gzip(data_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11453845it [7:45:05, 410.44it/s]\n"
     ]
    }
   ],
   "source": [
    "places_ids = places.gPlusPlaceId.values\n",
    "fr=open(data_path + \"reviews.clean.json\")\n",
    "fw=open(data_path + \"reviews.json\", \"w\")\n",
    "written_firstline = 0\n",
    "for line in tqdm(fr):\n",
    "    json_dat = ast.literal_eval(line)\n",
    "    if json_dat['gPlusPlaceId'] in places_ids: # only get reviews of businesses in the places file\n",
    "        if written_firstline == 0:\n",
    "            fw.write(\"[\")\n",
    "            json.dump(json_dat, fw)\n",
    "            written_firstline += 1\n",
    "        else:\n",
    "            fw.write(\",\\n\")\n",
    "            json.dump(json_dat, fw)\n",
    "                 \n",
    "fw.write(\"]\")\n",
    "\n",
    "fw.close()\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert file from json to csv\n",
    "with open(data_path + \"reviews.json\", \"r\") as f:\n",
    "    content = json.loads(f.read())\n",
    "\n",
    "df = pd.DataFrame(content)\n",
    "df.to_csv(data_path + \"reviews.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Final remarks\n",
    "Now that the data is downloaded it is ready to be prepared for analysis. This is done in the [data processing notebook](./DataProcessing.ipynb) which is the next suggested step to learn more about the project."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e9e982dfc258e8d223c37f4327606c8da7dedee0f3f31349257777800bfa443"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
